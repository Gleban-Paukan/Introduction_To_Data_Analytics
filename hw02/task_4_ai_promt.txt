################# Уважаемый проверяющий, если вы это читаете. Сначала я приложу текст, на который опирался в качестве основного (помимо оригинальной статьи и лекции по NLP https://www.youtube.com/watch?v=UUEEk35V5CE&ab_channel=girafe-ai) После неё будет общение и уточнения. Я так сделал для того, чтобы вы хоть что-то смогли прочитать (я попросил гпт отдельно переписать текст в читаемый вид. Он переписал немного сжато. Но это всё равно достаточно подробно)


Постановка задачи – объясню как простым языком, так и на математическом уровне.
Структура входных данных – опишу их тип, формат и представление.
Ограничения на данные – разберу возможные требования к данным.
Эффективность предложенной модели – рассмотрю, насколько хорошо она решает задачу и какие метрики использовались.
Необходимые знания – выделю теоретические и практические навыки, необходимые для понимания статьи.
Дополнительные детали – разберу архитектуру модели, обучение, сравнение с альтернативами и практическое применение.
Я проведу глубокий анализ статьи и предоставлю полный отчет с выводами. Ожидайте результатов вскоре!
Research completed in 9m· 33 источников
1. Постановка задачи
-----------------------
Простое объяснение. Статья посвящена задаче машинного перевода, то есть автоматического преобразования текста с одного
языка на другой. Например, модель должна читать предложение на английском и генерировать его эквивалент на русском. Главная
трудность в том, что смысл каждого слова зависит от контекста – чтобы правильно перевести фразу, нужно учесть взаимосвязи
между всеми словами. Традиционно для этого использовались сложные рекуррентные нейросети, последовательно
обрабатывающие слова одно за другим, дополняемые механизмом внимания (attention). Авторы же формулируют новую идею:
можно полностью отказаться от рекуррентности и свёрток и решать задачу перевода, опираясь только на механизм внимания
между частями последовательности. Проще говоря, модель “смотрит” на всё входное предложение сразу и учится выделять, какие
слова взаимосвязаны, чтобы на этой основе поэтапно породить правильный перевод. Такой подход обещает упростить архитектуру
и повысить скорость обучения и работы модели.
Математическая формулировка. Формально задача – это моделирование условного распределения вероятностей
последовательностей. Дан входной текст, представимый как последовательность символов/слов 
 = (, , …, ), требуется сгенерировать выходную последовательность 
 = (, , …, ) на другом языке. Модель строится по схеме «энкодер–декодер»: энкодер отображает вход  в набор непрерывных 
 = (, …, ), а декодер, получив , порождает выход  по одному элементу за раз. При генерации каждый следующий токен 
подбирается с учётом всего входного предложения и уже сгенерированной части выхода (, …, ). Таким образом, модель
аппроксимирует распределение (| ), разложенное в произведение условных вероятностей токенов:
(, …,  | , …, ) = ∏^ ( | , …, , , …, ).
Обучение сводится к максимизации лог-правдоподобия правильных переводов из обучающей выборки (минимизации кроссэнтропии). После обучения для нового входного предложения модель должна выдавать такую последовательность , которая
максимизирует эту условную вероятность (либо близка к эталону по метрикам качества перевода вроде BLEU).
2. Структура входных данных
---------------------------
Тип данных. В работе используется текстовая последовательность (естественный язык). Конкретно, модель обучается на
параллельных корпусах предложений для перевода: входом является предложение на исходном языке (например, английском), а
правильным выходом – соответствующий ему перевод на целевой язык (например, немецкий или французский). Таким образом,
каждое обучающее примеро – это пара строк текста. В более общем плане, Transformer рассчитан на произвольные
последовательные данные (слова, субсловные токены, символы и т.п.), и авторы показывают, что модель может успешно
применяться не только к переводу, но и к другим последовательностям (например, синтаксическим структурам).
Формат представления. Поскольку нейросеть не может напрямую работать с текстом, каждое слово (токен) кодируется численно.
Слова разбиваются на токены с помощью токенизации (для перевода авторы применяли Byte-Pair Encoding, об этом ниже) и
каждому токену ставится в соответствие индекс из словаря. Далее эти индексы преобразуются в эмбеддинги – обучаемые
вещественные векторы фиксированной размерности model. На вход энкодера подаётся последовательность таких эмбеддингов
(плюс позиционная информация – см. детали архитектуры), то есть матрица размером: длина последовательности × размер
эмбеддинга. Выход декодера генерируется токен за токеном: на каждом шаге декодер предсказывает следующий токен , для чего
используется финальный полносвязный слой и softmax над словарём. Для обучения используется метод teacher forcing: известная
правильная последовательность  подаётся декодеру с небольшим смещением (специальный символ начала последовательности
добавляется перед ), чтобы модель училась предсказывать следующий токен по предыдущим. Таким образом, входными
данными модели являются пары параллельных последовательностей (например, предложение на языке A и то же предложение на
языке B), представленные в виде последовательностей токен-индексов/эмбеддингов; выходными данными (для обучения) –
правильные последовательности-переводы, а выход модели – сгенери
3. Ограничения на данные
-------------------------
При подготовке данных для Transformer следует учитывать несколько моментов:
Объём и разнообразие данных. Модель содержит десятки миллионов параметров, поэтому для её обучения требуется большой
корпус примеров. В экспериментах использовались обширные наборы: ~4.5 млн пар предложений для английского-немецкого и 36
млн для английского-французского. Важно, чтобы данные охватывали разнообразные конструкции и словарный запас языков,
иначе модель может не выучить редкие обороты. Авторы также показали, что Transformer способен обучаться и на более небольших
датасетах без потери конкурентоспособности – например, успешно применён к задаче парсинга с ограниченным числом примеров,
где рекуррентные seq2seq-модели ранее не достигали высокого качества.
Баланс классов и редкие слова. Задача перевода не формулируется как классификация на фиксированное число классов, поэтому
типичный вопрос о балансе классов напрямую не стоит. Тем не менее, можно рассматривать каждый возможный выходной токен
как “класс” при предсказании следующего слова. Распределение слов в языке весьма неравномерное (некоторые слова
встречаются часто, другие редко), и без специальных мер модель могла бы плохо справляться с редкими словами. Для решения
этой проблемы авторы применили технику Byte-Pair Encoding (BPE) для токенизации. Этот метод разбивает слова на более мелкие
единицы (например, субслова), благодаря чему даже редкое или неизвестное слово представляется последовательностью
известных токенов. BPE ограничивает размер словаря – в эксперименте ~37 тысяч общих токенов для пар EN–DE и ~32k для EN–FR
– и практически устраняет ситуацию OOV (“out of vocabulary”, вне словаря). Таким образом, модель не сталкивается с совершенно
неизвестными символами при переводе: любой входной токен можно разложить на субтокены из обученного словаря.
Пропущенные значения. В параллельных данных не должно быть отсутствующих элементов – каждому входному предложению
необходим соответствующий перевод. Если в датасете имеются пары с пустым переводом или пустым оригиналом, их следует
удалить при препроцессинге. Также нежелательны сильно “шумные” или некорректно выровненные пары (например, если перевод
не соответствует оригиналу), так как они вводят модель в заблуждение. Transformer не предусматривает явной обработки пропусков
значений внутри последовательности – отсутствующие слова можно было бы пометить специальным токеном, но такой ситуации
стараются избегать очисткой данных. При составлении батчей для обучения более короткие предложения обычно дополняются
специальными <pad> токенами до длины самого длинного в батче; эти padding токены игнорируются моделью с помощью масок
внимания, чтобы не влиять на результат.
Длина и формат последовательностей. Входные и выходные последовательности могут иметь разную длину ( и ). Transformer
способен обрабатывать последовательности произвольной длины, ограниченной лишь памятью и выбранной длиной позиционного
кодирования. Важное ограничение – очень длинные последовательности: вычислительная сложность механизма внимания растёт
квадратично от длины (поскольку внимание учитывает пары токенов). В экспериментальном корпусе длина предложений была
умеренной, и эту проблему смягчали пакованием предложений по схожей длине: в каждой обучающей пачке содержалось около 25k
токенов, разбитых на предложения примерно равной длины. Это позволяет эффективно загрузить вычисления, избегая ситуации,
где одна очень длинная последовательность в батче замедляет всё обучение. Если бы пришлось переводить очень длинные тексты
(например, документы с тысячами слов), потребовались бы дополнительные меры – авторы отмечают, что можно ограничивать
поле зрения внимания фиксированным окном по соседству токена, чтобы снизить расчётные затраты на длинных
последовательностях (в ущерб учёту дальнего контекста). В данной работе модель обучалась на предложениях стандартной длины
(до нескольких десятков слов после BPE), поэтому специальных ограничений на длину, кроме пакетирования по длине, не
вводилось.
Формат и качество разметки. Все данные перед подачей модели должны быть приведены к единообразному формату токенов. Для
этого исходные предложения нормализуют (напр., приводят к одному регистру, удаляют странные символы), токенизируют и
применяют BPE. Важно использовать один и тот же словарь токенов для обучающих, проверочных и тестовых данных, чтобы
модель не встретила токенов, которых не видела при обучении. Также для модели перевода полезно валидационное множество
(development set) – набор пар предложений, не участвующих в обучении, на которых можно периодически измерять качество
(перплексию, BLEU) и настраивать гиперпараметры. В статье, например, упоминается использование newstest2013 как dev-сета для
отслеживания качества и подбора некоторых гиперпараметров, а newstest2014 – как тестового набора для финальной оценки.
4. Решает ли предложенная модель поставленную задачу?
-----------------------------------------------------
Метрики качества. Для оценки качества перевода авторы используют стандартную метрику BLEU (Bilingual Evaluation Understudy).
BLEU измеряет точность перевода, сравнивая n-граммы с эталонным человеческим переводом; значение выражается в процентах
соответствия. В машинном переводе даже лучшие модели редко набирают BLEU выше 50–60, а улучшение на несколько пунктов
считается значительным скачком качества. Помимо BLEU, в ходе обучения мониторилась perplexity (перплексия) на дев-выборке –
мера, связанная с лог-вероятностью правильных последовательностей (чем меньше, тем модель увереннее и точнее). Однако
итоговые выводы о success модели делаются по BLEU на тестовых данных, так как именно BLEU коррелирует с тем, насколько
хорошо модель переводит. Также авторы сравнивают время и стоимость обучения разных моделей (число операций FLOPs), чтоб
подтвердить эффективность Transformer.
Результаты. Transformer на момент публикации установил новые рекорды качества перевода. В задаче перевода WMT 2014 EnDe
модель Transformer (большая конфигурация) достигла 28.4 BLEU, что более чем на 2 BLEU выше предыдущего лучшего результата
(даже с учётом ансамблей моделей). Для пары WMT 2014 EnFr односеточная модель Transformer показала 41.8 BLEU, превзойдя все
существовавшие на тот момент одиночные модели и сопоставившись с качеством ансамблей. Эти числа означают, что переводы,
генерируемые новой моделью, ближе к человеческому переводу, чем переводы, полученные старыми архитектурами – модель
успешно решила поставленную задачу, улучшив качество на заметную величину. Кроме того, достигнуть такого результата удалось с
меньшими затратами: обучение большого Transformer заняло 3.5 дня на 8 GPU, что составляет лишь малую долю ресурсов,
затраченных предыдущими лучшими моделями. Даже базовая версия Transformer (с меньшим числом параметров) превзошла по
BLEU все ранее опубликованные модели и ансамбли, при этом требуя значительно меньше вычислительных операций на обучение.
Это подтверждает не только высокую точность, но и эффективность предложенного подхода. Также примечательно, что архитектура
показала свою универсальность: её успешно применили к задаче синтаксического разбора предложений и получили конкурентное
качество парсинга, что говорит о способности модели решать и другие задачи последовательностей, помимо перевода.
5. Необходимые знания для понимания работы модели
--------------------------------------------------
Для полного понимания устройства и обучения модели Transformer требуются знания из нескольких областей математики,
программирования и ML:
Линейная алгебра. Необходимо уверенно работать с понятиями векторов и матриц, так как вся модель описывается матричными
операциями. Например, эмбеддинги токенов – это векторы, слои внимания – это умножения матриц запросов и ключей и т.д.
Понимание матричного умножения, транспонирования, размерностей тензоров – обязательная база. Также полезно знание базовых
преобразований вроде синусоид, поскольку позиционное кодирование реализовано через синусоиды.
Высшая математика (анализ) и оптимизация. Как и все нейросети, Transformer обучается градиентным спуском, поэтому нужно
понимать, что такое производная и градиент, как происходит обратное распространение ошибки. Знание основ математической
оптимизации поможет разобраться, почему используются определённые приёмы (например, алгоритм Adam для обновления весов,
специальный график изменения learning rate и т.д.). Без этого тяжело понять тонкости обучения глубокой модел
Теория вероятностей и информация. Поскольку модель порождает вероятностное распределение по возможным переводам, важно
разбираться в вероятностных моделях. Следует понимать, что такое условная вероятность (| ), как она факторизуется по шагам.
Знакомство с понятием энтропии и кросс-энтропии необходимо для понимания функции потерь, на основе которой происходит
обучение. Также нужна интуиция, как работает softmax: модель выдаёт “сырые” оценки (логиты) для каждого слова, а softmax
превращает их в вероятности следующего токена. Знание метрики BLEU и принципов оценки качества перевода тоже будет полезно,
чтобы понять, как интерпретировать результаты.
Основы нейронных сетей и архитектур НС. Transformer сочетает ряд стандартных компонентов нейросетей – полноcвязные слои,
нормализацию, dropout, residual-коннекты. Нужно представлять, что из себя представляют эти механизмы и зачем они нужны
(например, residual-связи облегчают обучение глубоких сетей). Также желательно быть знакомым с классической архитектурой
“энкодер–декодер” с механизмом внимания, которая применялась до Transformer на базе рекуррентных сетей. Исторически, перед
появлением трансформеров, доминировали последовательные RNN-модели (LSTM, GRU) с вниманием для перевода. Понимание их
устройства и ограничений (например, что RNN обрабатывает слова по одному и сохраняет состояние) помогает оценить, какие
улучшения привнёс Transformer. В частности, полезно знать работу внимания Bahdanau (2015) или Luong (2015) в seq2seq моделях,
поскольку “Attention is All You Need” развивает эту идею, сделав внимание центральным элементом всей архитектуры.
Навыки программирования. Хотя концептуально модель можно понять теоретически, для практического применения нужно
уверенное владение фреймворками глубокого обучения (TensorFlow, PyTorch и др.) и языком Python. Transformer – довольно сложная
система с множеством матричных операций; умение читать и писать код модели (например, реализацию слоя многоголового
внимания, masking, обработку батчей) существенно помогает в понимании. Полезно знать формат данных в коде – например, что
входной тензор имеет размерность [batch, seq_len, d_model], где batch – число предложений в пачке. Также знание библиотек для
обработки текста (токенизация, Byte-Pair Encoding) необходимо, чтобы правильно сформировать входы для модели. Таким образом,
чтобы вникнуть в детали реализации Transformer, требуются навыки программирования на практике, помимо теоретических знаний.
6. Дополнительные детали
--------------------------
Архитектура модели Transformer. Модель представляет собой классический энкодер–декодер, то есть состоит из двух основных
модулей: энкодер (слева на схеме) обрабатывает входную последовательность, а декодер (справа) генерирует выходную. Отличие от
предыдущих seq2seq-моделей в том, что и энкодер, и декодер построены исключительно на механизме самовнимания (self-attenti
и позиционно-независимых преобразованиях, без рекуррентных или сверточных слоёв. Энкодер состоит из $N$ одинаковых слоёв
(в оригинале $N=6$ для базовой модели). Каждый такой слой содержит два последовательных подтакта: первый – это
многоголовое самовнимание (multi-head self-attention), второй – позиционно-применяемый полностью связанный слой (feedforward). После каждого подтакта выход прибавляется к исходному входу этого подтакта (residual connection) и нормализуется по
слоям – это помогает стабилизировать и ускорить обучение. Чтобы residual-суммирование было корректным, все подслои и
эмбеддинги имеют одинаковую размерность выхода $d_{\text{model}}$ (например, 512). Декодер тоже состоит из $N$ одинаковых
слоёв, но в каждом добавлен третий подтакт внимания к выходам энкодера. Таким образом, каждый слой декодера выполняет: (1)
многоголовое самовнимание над уже сгенерированной частью выхода, (2) многоголовое внимание к выходу энкодера (т.н. encoderdecoder attention), и (3) полносвязный feed-forward слой. После каждого – residual-суммирование и layer normalization, аналогично
энкодеру. В самовнимании декодера вводится маскировка будущих позиций: модель не должна видеть последующие токены,
которые ещё не сгенерированы. Реализуется это занулением внимания к токенам с позиции > $t$ при расчёте для позиции $t$.
Кроме того, перед подачей в декодер выходная последовательность сдвигается на один шаг (добавляется специальный токен
начала, последний токен отбрасывается), благодаря чему декодер на шаге $t$ получает в качестве своего входа все предыдущие
правильные токены $y_{<t}$, но не знает будущих $y_{>t}$. Эти меры обеспечивают авторегрессию: предсказание для позиции $t$ не
зависит от истинных слов на позициях $>t$, то есть генерируется слева-направо, как и требовалось.
Механизм внимания (attention). Ключевой компонент модели – это механизм “внимания”, позволяющий обращаться к
произвольным частям последовательности при обработке каждого элемента. В общем виде attention принимает на вход три набора
векторов: запросы (queries), ключи (keys) и значения (values). В случае самовнимания и запросы, и ключи, и значения сгенерированы
из одних и тех же входов (например, скрытых представлений слов предыдущего слоя) – то есть модель “посылает запрос” от
каждого слова к самим себе и соседям. В слое encoder-decoder attention запросы идут из декодера, а ключи/значения – из выходов
энкодера (так декодер может фокусироваться на нужных местах входного предложения при генерации каждого слова). Алгоритм
Scaled Dot-Product Attention внутри слоя работает так: для данного запроса вычисляются скалярные произведения со всеми
ключами; каждое произведение делится на  (корень из размерности ключа) – это необходимое масштабирование, предохранение
от слишком больших значений скалярного произведения; затем через softmax превращается в веса, которые показывают,
насколько данный запрос “внимателен” к каждому возможному значению. Эти веса затем используются для взвешивания
соответствующих значений и суммирования их – результат и будет выходным представлением для данного запроса. Формула
можно записать так: 
Attention(Q,K,V) = softmax((QK) / √) V.
Вектор на выходе attention содержит собранную информацию из тех позиций, которым запрос уделил наибольшее внимание.
Важное преимущество – дистанция между любыми двумя токенами не имеет ограничения: даже пару очень удалённых слов
внимание свяжет напрямую через одно скалярное произведение. Это контрастирует с рекуррентными и свёрточ
архитектурами, где, чтобы связать дальние слова, нужно пройти через много промежуточных шагов или слоёв.
Многоголовое внимание. В Transformer каждый attention-слой дублируется несколько раз параллельно – это называется Multi-Head
Attention (многоголовое внимание). Идея в том, что вместо одного пространства признаков внимание будет происходить в
нескольких подпространствах. Исходные векторы запроса, ключа и значения с помощью обучаемых матриц проецируются в $h$
разных пространств меньшей размерности (например, $h=8$ голов по 64 измерения каждая, так что суммарная размерность
остаётся $8×64=512$). В каждом из этих пространств выполняется свой расчет внимания (со своими весами и результатами). Затем
выходы от всех “голов” конкатенируются и проецируются обратно в пространство размерности $d_{\text{model}}$. Таким образом,
модель как бы имеет $h$ разных “фокусов внимания”, которые могут параллельно изучать различные аспекты связей между
словами. Например, одна голова может сосредоточиться на согласовании существительных и глаголов, другая – на соответствии
имён собственных, третья – на соответствии порядка слов и т.д. Это эмпирически оказалось намного эффективнее, чем одно
внимание полной размерности: многоголовое внимание позволяет модели совместно учитывать разные типы зависимостей в
предложении. После многоголового attention-слоя обычно идёт полносвязный (Feed Forward) слой, применяемый отдельно к
каждому положению (то есть одинаково ко всем позициям). Он состоит из двух линейных преобразований с функцией активации
ReLU между ними. Размерность внутреннего слоя больше, чем входная (в базовой модели 2048 против 512), что позволяет ему
нелинейно преобразовывать данные, обогащая представления токенов. Поскольку этот слой не перемешивает информацию между
позициями, все взаимосвязи между словами учатся именно на этапе attention; FFN же отвечает за локальную обработку каждого
токена (например, выстраивание более сложных признаков на основе собранной через внимание информации).
Эмбеддинги и позиционное кодирование. На самом нижнем уровне и энкодер, и декодер принимают на вход эмбеддинги токенов –
обучаемые векторы, соответствующие словам. В начале обучения эти эмбеддинги инициализируются случайно и далее
подстраиваются под задачу перевода. Размерность эмбеддинга равна размерности модели $d_{\text{model}}$, чтобы его можно
было напрямую подавать в первый attention-слой. Выход декодера (последнего слоя) через линейный слой той же размерности
проецируется обратно в пространство размерности словаря, после чего применяется softmax для получения вероятностей слов. В
Transformer применена интересная техника – совместное использование весов (weight sharing) между embedding-слоями и
выходным линейным слоем декодера. Это означает, что матрица эмбеддингов входного языка, матрица эмбеддингов выходного
языка и матрица, используемая для предсказания слова из скрытого состояния, – это по сути одна и та же матрица (разумеется,
размер словаря входа и выхода должен совпадать; в эксперименте использовали единый словарь субслов BPE для обоих языков).
Такая схема экономит параметры и, как показали предыдущие работы, улучшает качество, потому что модель как бы учит общее
пространство для “вхождения” слова и “выхода” слова. Помимо эмбеддингов, к каждому токену добавляется его позиционный
признак. Поскольку в архитектуре нет рекуррентности и свёрток, модель сама по себе не “знает”, какой токен первый, а какой
второй. Поэтому вектор позиционного кодирования добавляется к эмбеддингу токена, вводя информацию о позиции в
последовательности. Авторы исследовали разные варианты таких кодирований: можно обучать отдельный вектор для каждой
позиции (как дополнительный embedding), либо задать фиксированную функцию. В итоге выбрали фиксированное синусоидальное
позиционное кодирование. Идея в том, что для каждой позиции  и каждого измерения  генерируется значение по формуле:
(, 2) = sin( / 10000^(2/model)),
(, 2+1) = cos( / 10000^(2/model)),
то есть разные частоты синуса/косинуса для разных компонент вектора. Такие колебания заданы так, что относительное смещение
между позициями отражается сдвигом фаз: модель может через линейную комбинацию синусоид узнать, на сколько позиций два
слова отстоят друг от друга. Важное свойство синусоид – их можно экстраполировать на позиции, которых не было в обучении (в
отличие от обучаемых позиций, где фиксированный словарь позиций ограничен). Авторы отметили, что если сделать позиции
обучаемыми векторами, качество не ухудшилось, однако выбрали синусоиды как простое и элегантное решение, не требующее
дополнительных параметров. В результате, на вход каждого attention-слоя поступают обогащённые эмбеддинги: токеновый
эмбеддинг + позиционный вектор (эти два складываются покомпонентно).
Регуляризация и тренинг. Transformer – глубокая модель, поэтому для успешного обучения применены несколько техник
регуляризации. Во-первых, используется Dropout: с определённой вероятностью нейрон “отключается”, то есть обнуляется
активация. Dropout применяется после каждого attention- и feed-forward подслоя (перед сложением с residual) и к сумме эмбеддинга
с позиционным кодированием на входе энкодера и декодера. Вероятность dropout выбрана 0.1 (10%) для базовой модели. Вовторых, используется Label Smoothing – сглаживание меток на выходе. При вычислении loss вместо “жёсткого” one-hot
распределения для целевого слова берётся распределение, где правильному слову присваивается вероятность 1−ε (например, 0.9),
а небольшой остаток ε распределяется по другим словам. В эксперименте взяли ε=0.1. Такая небольшая неопределённость в
целевых метках мешает модели переуверенно подгоняться под тренировочные данные (что могло бы привести к переобучению).
Интересно, что label smoothing немного ухудшает перплексию на дев-сете (поскольку модель намеренно не доучивает до нуля
ошибку на тренировочных примерах), но заметно повышает BLEU на тесте – т.е. модель обобщает лучше, делая меньше грубых
ошибок в переводе. В-третьих, при генерации перевода на инференсе авторы использовали beam search с шириной бима 4 и
штрафом за длину последовательности. Beam search проходит по пространству возможных переводов, удерживая топ-4 наиболее
перспективных варианта и выбирая из них лучший полный перевод – это стандартный приём, повышающий качество выходного
текста. Наконец, для обучения использовался оптимизатор Adam с специальным расписанием изменения learning rate: сначала LR
линейно увеличивался в течение 4000 первых шагов, затем уменьшался пропорционально 1/√step. Такой прогрев (warm-up) и
последующее затухание шага обучения обеспечили более стабильное и быстрое сходжение модели. Базовая модель обучалась 100k
шагов (~12 часов) на 8 GPU, большая – 300k шагов (~3.5 дня). После обучения для итоговой модели брали усреднение последних
нескольких чекпоинтов (5 для base, 20 для big) для чуть более устойчивого результата. Все эти ин
регуляризация, декодирование) в совокупности позволили получить максимально качественную и генерализованную модель
перевода.
Сравнение с альтернативными подходами. Перед появлением Transformer в задачах машинного перевода господствовали
рекуррентные сети с вниманием и свёрточные seq2seq-модели. Рекуррентные архитектуры (LSTM/GRU) хорошо умели хранить
контекст, но не позволяли обрабатывать разные слова параллельно, так как состояние RNN обновляется последовательно от
первого слова к последнему. Это усложняет масштабирование: даже при распараллеливании по батчам, длина предложения
остаётся фактором, замедляющим обучение. Transformer радикально устранил эту проблему – благодаря self-attention все элементы
последовательности обрабатываются одновременно, зависимость между любыми двумя словами вычисляется за один шаг
внимания, и модель хорошо распараллеливается на современном оборудовании (GPU/TPU). Например, Transformer достиг
рекордного качества за считанные дни обучения, тогда как сравнимые рекуррентные модели требовали недель обучения или
огромных ансамблей моделей. Альтернативой RNN в 2017 году были сверточные seq2seq-модели (ByteNet, ConvS2S и др.), которые
тоже снимали ограничение последовательности с помощью свёрток по времени. Однако у свёрток есть свой недостаток: чтобы
“увидеть” дальний контекст, нужно несколько сверточных слоёв – по сути, число операций между двумя токенами растёт линейно
или логарифмически от расстояния. Transformer же устанавливает прямую связь между любыми двумя токенами через внимание,
без увеличения числа операций с ростом расстояния (правда, за это приходится платить вниманием ко всем парам сразу, что
накладно по памяти). Ещё один плюс – простота архитектуры. В Transformer нет сложных рекуррентных переходов, следовательно
градиенты легче протекают в глубоких сетях, нет проблемы vanishing/exploding градиентов, решаемой ранее специальными
воротами LSTM. Структурно трансформер очень модульный: одинаковые слои повторяются N раз, и каждый слой состоит всего из
нескольких стандартных операций (attention, matmul, ReLU, norm). Это упростило исследование: авторы легко пробовали разные
варианты (например, разное число голов, размерности, варианты нормализации) и наблюдали эффект в таблицах экспериментов.
Multi-head внимание само по себе оказалось новшеством, компенсирующим возможные ограничения одного внимания. Если бы мы
брали одно скалярное произведение большого размера, это равносильно усреднённому вниманию по всем признакам, что может
смазывать важные зависимости. В многоголовом механизме же разные “головы” могут выучивать разные шаблоны связей, и
суммарно модель захватывает больше информации. В итоге Transformer не просто теоретически проще – он показал себя сильнее
на практике: качество перевода улучшилось относительно всех конкурентов (RNN, CNN) на тех же тестах, причём даже без ансамбля
моделей. При этом вычислительные затраты на достижение такого качества оказались меньше: благодаря полной параллелизации
по словам модель обучилась быстрее всех (например, новый SOTA достигнут за ~12 часов, тогда как предшественники тренировали
модели куда дольше). Таким образом, предложенная архитектура доказала своё преимущество над альтернативами, а главное –
заложила базу для дальнейшего прогресса в NLP, показав, что можно отказаться от рекуррентности и добиться лучшего результата.
Практическое применение и улучшения. Архитектура Transformer сразу после публикации стала крайне востребованной в сфере
обработки текста. Фактически, она стала новым стандартом: многие последующие модели строились на основе энкодера или
декодера трансформера. Например, знаменитые языковые модели BERT и GPT напрямую основаны на идеях из “Attention is All You
Need”: BERT представляет собой исключительно энкодерную часть (self-attention слои, обученные предсказывать пропущенные
слова), а GPT – исключительно декодерную (генеративная модель текста). Оба они достигли выдающихся результатов, подтвердив
универсальность трансформер-архитектуры. Более того, принципы self-attention оказались применимы не только для языка – их
переняли в компьютерном зрении (Vision Transformer для обработки изображений), в аудио, в моделях для биологических
последовательностей и др. Благодаря хорошей параллелизации трансформеры позволили создать очень большие модели (сотни
слоёв и миллиард+ параметров), обучаемые на громадных данных – это привело к появлению современных GPT-3, GPT-4, PaLM и
других систем искусственного интеллекта. Что касается самой статьи Vaswani et al. (2017), они продемонстрировали и
переносимость модели на другую NLP-задачу – синтаксический разбор предложений (constituency parsing). Transformer успешно
справился с разбором даже при ограниченном объёме обучающих данных, превзойдя качество рекуррентных подходов. Это
показало, что механизм внимания может эффективно учить структурные зависимости в последовательностях разного рода.
6. Необходимые знания для понимания работы модели
--------------------------------------------------
(См. пункт 5 выше.)
7. Residual-коннекты (skip connections)
-----------------------------------------
Residual-коннекты – это механизмы, позволяющие передавать входные данные слоя напрямую к его выходу. Идея заключается в
том, чтобы вместо обучения сложного преобразования H(x) обучалась остаточная функция F(x) = H(x) – x. Тогда итоговый выход
вычисляется как:
    H(x) = F(x) + x
Такой подход помогает:
- Улучшить распространение градиентов в глубоких сетях, предотвращая проблему исчезающих градиентов.
- Сохранить важную информацию между слоями.
- Ускорить и стабилизировать обучение.
Если размерности входа и F(x) не совпадают, применяется дополнительное линейное преобразование (например, y = F(x) + Wx) для
согласования размерностей.
Residual-коннекты впервые были популяризированы в архитектуре ResNet и успешно применяются в Transformer для каждого
подтакта, что помогает модели обучаться более эффективно









#################### Это практически точь-в-точь тот текст, который мне выдал чат гпт после глубокого исследования.

Мне не совсем понятно про Длина и формат последовательностей.

Распиши подробнее. Также распиши подробнее суть и примеры BPE, как именно он разделяет слова. Он пытается разделить их на морфемы, пытаясь выделять корень, согласно каким-то эвристическим правилам, или просто фиксированно по несколько букв? 
Reasoned about последовательности и BPE для 10 секунд
Давайте подробнее разберём два аспекта:

1. Длина и формат последовательностей
Длина последовательности:
В контексте NLP (обработки естественного языка) последовательность – это упорядоченный набор токенов (слов, субслов или символов), который подаётся на вход модели. В статье Transformer каждое предложение представлено как последовательность токенов, и длина такой последовательности может варьироваться от нескольких до, возможно, десятков (или более) токенов. На практике:

Ограничение длины: В большинстве экспериментов используется фиксированное максимальное значение длины. Если предложение короче этого максимума, оно дополняется специальными токенами (например, <pad>) до нужной длины, чтобы все последовательности в одном батче имели одинаковую длину. Это удобно для параллельной обработки на GPU.
Позиционное кодирование: Так как модель Transformer обрабатывает все токены параллельно и не имеет явного порядка, к каждому эмбеддингу добавляется позиционное кодирование, которое сообщает модели положение токена в последовательности. Позиционное кодирование обычно задаётся либо фиксированными (например, синусоидальными), либо обучаемыми векторами. Это помогает модели учитывать порядок слов, несмотря на параллельную обработку.
Формат представления:
Каждое слово или токен сначала преобразуется в числовой идентификатор через процесс токенизации. Затем эти идентификаторы отображаются в эмбеддинги — вещественные векторы фиксированной размерности 
𝑑
model
d 
model
​
 . Итоговая последовательность представляется в виде матрицы:

Размерность матрицы: 
[
𝑛
,
𝑑
model
]
[n,d 
model
​
 ], где 
𝑛
n – количество токенов в предложении, а 
𝑑
model
d 
model
​
  – размерность эмбеддинга.
Padding: Для формирования батчей (группы последовательностей) все предложения доводятся до одинаковой длины путём добавления специальных токенов, значение которых игнорируется при вычислении внимания (благодаря маскам).
Такая схема позволяет эффективно обрабатывать переменные длины входных данных, сохраняя порядок и учитывая зависимость между токенами посредством механизма внимания.

2. Byte-Pair Encoding (BPE)
Суть алгоритма BPE:
BPE — это алгоритм, изначально разработанный для сжатия данных, который адаптировали для токенизации текста. Идея заключается в том, чтобы разбить слова на более мелкие единицы (субслова) таким образом, чтобы:

Часто встречающиеся последовательности символов остались целыми (например, распространённые корни или суффиксы).
Редкие или неизвестные слова можно было бы разбить на знакомые субсловные единицы, что позволяет модели работать с словами, которых не было в обучающем словаре (out-of-vocabulary, OOV).
Как работает BPE:

Начальное представление:
Каждое слово из корпуса представляется как последовательность отдельных символов плюс специальный символ конца слова (или разделитель). Например, слово «lower» представляется как:
l o w e r</w>
Здесь </w> служит маркером конца слова.

Подсчёт пар:
Алгоритм подсчитывает все пары последовательных символов (например, "l o", "o w", "w e", "e r", "r </w>") и определяет, какая пара встречается наиболее часто во всём корпусе.

Слияние пары:
Затем наиболее частая пара заменяется на новый токен, который объединяет два символа. Если, например, пара «l o» является самой частой, то все её вхождения заменяются на токен «lo».
Таким образом, слово «lower» может преобразоваться в:
lo w e r</w>

Итеративный процесс:
Процесс повторяется: на каждом шаге алгоритм ищет самую частую пару в обновлённом наборе токенов (которые уже могут состоять из символов или объединённых групп) и сливает её в один токен. Это продолжается до достижения заданного размера словаря (например, 30–40 тысяч токенов).

Примеры работы BPE:

Пример 1:
Пусть в корпусе часто встречается слово «low» как отдельное целое. В начале слово «lower» представлено как l o w e r</w>. Если пара l o часто встречается, она сливается в токен «lo». Затем, возможно, «lo w» тоже сливается в «low». В итоге «lower» может получиться как:
low e r</w>
Если далее пара «e r» также часто встречается, её слияние даст токен «er»:
low er</w>
Таким образом, слово «lower» разбивается на два субслова: «low» и «er».

Пример 2:
Для слова «newest» начальное представление будет:
n e w e s t</w>
Если пара n e сливается в «ne», а затем, например, пара w e сливается в «we», то в итоге слово может получиться как:
ne we s t</w>
При дальнейших слияниях алгоритм может оставить «new» как единое целое, а оставшуюся часть разбить на «est», получая:
new est</w>
Таким образом, модель сможет работать и со словами, которые она не встречала ранее, разбивая их на знакомые субслова.

Морфемный анализ или эвристика?
BPE не пытается явно выделять морфемы или проводить лингвистический разбор слова (то есть определять корень, приставку, суффикс). Алгоритм полностью опирается на статистику – он сливает те последовательности символов, которые чаще всего встречаются вместе. В результате получаются субслова, которые зачастую совпадают с морфемами, особенно в языках с относительно прозрачной морфологией, но это не гарантируется. В некоторых случаях разбиение может не соответствовать классическим морфологическим границам, а просто отражать частотность определённых последовательностей символов.

Эвристика, основанная на частотности:
Алгоритм делает выбор, основываясь на том, какая пара встречается чаще всего, без лингвистического анализа. То есть он не имеет заранее заданных правил для определения корня слова или других морфем – всё определяется статистически на обучающем корпусе.

Параметр – размер словаря:
Размер конечного словаря (количество субсловных единиц) выбирается заранее. Если размер словаря велик, то многие слова сохранятся целиком; если он мал, то слова будут разбиваться на более мелкие части. Таким образом, параметр задаёт компромисс между полнотой представления и способностью обрабатывать редкие или новые слова.

Итог
Длина и формат последовательностей:
В Transformer каждое предложение представляется как последовательность токенов, преобразованных в эмбеддинги фиксированной размерности. Последовательности дополняются (padding) до равной длины в батче и снабжаются позиционным кодированием, которое сообщает моделью порядок токенов. Это позволяет эффективно обрабатывать переменные по длине входные данные и учитывать отношения между токенами независимо от их позиции в последовательности.

Byte-Pair Encoding (BPE):
BPE – это метод токенизации, который итеративно сливает наиболее частые пары символов (или уже объединённых токенов) в один субсловный элемент до достижения нужного размера словаря. Он не проводит явного морфологического разбора, а использует эвристику, основанную на частотном анализе. Результатом является набор субсловных единиц, который позволяет эффективно представлять как часто встречающиеся слова целиком, так и разбивать редкие или новые слова на знакомые компоненты.

Эти механизмы вместе позволяют модели Transformer гибко работать с текстовыми данными, обеспечивая высокое качество перевода даже при наличии ограничений по памяти и разнообразии входных предложений.






Вы сказали:
residual-коннекты это что? распиши 
Рассуждала для 6 секунд
Residual-коннекты (или residual-связи, skip connections) – это архитектурное решение в нейронных сетях, позволяющее "перепрыгивать" через один или несколько слоёв, то есть передавать информацию напрямую от входа блока к его выходу. Эта идея была впервые популяризирована в статье ResNet (He et al., 2015) и затем широко применялась во многих архитектурах, включая Transformer.

Основная идея
Вместо того, чтобы учить преобразование, которое переводит вход 
𝑥
x в какое-то новое представление 
𝐻
(
𝑥
)
H(x), residual-коннект позволяет блоку сети учить остаточную функцию 
𝐹
(
𝑥
)
=
𝐻
(
𝑥
)
−
𝑥
F(x)=H(x)−x. Тогда окончательное преобразование записывается как:

𝐻
(
𝑥
)
=
𝐹
(
𝑥
)
+
𝑥
H(x)=F(x)+x
То есть вместо изучения сложного преобразования 
𝐻
(
𝑥
)
H(x) модель обучается вычислять разницу между 
𝐻
(
𝑥
)
H(x) и 
𝑥
x. Это часто оказывается проще, особенно когда оптимальное 
𝐻
(
𝑥
)
H(x) близко к тождественной функции (то есть, когда 
𝐹
(
𝑥
)
F(x) должен быть близок к нулю).

Зачем нужны residual-коннекты
Улучшение распространения градиентов:
При обучении очень глубоких сетей часто возникает проблема исчезающих или взрывающихся градиентов. Residual-коннекты облегчают обратное распространение градиента, позволяя сигналу “обходить” сложные нелинейные преобразования и напрямую передаваться от более глубоких слоёв к ранним.

Ускорение и стабилизация обучения:
Добавление skip-соединений делает пространство поиска оптимальных параметров более гладким, что помогает модели быстрее сходиться во время обучения. Это особенно важно при использовании очень глубоких сетей, где прямое обучение без таких связей может быть крайне неустойчивым.

Лучшее сохранение информации:
Такие связи позволяют передать исходную информацию, которая может быть полезной на более поздних этапах. Например, если определённый признак важен, residual-коннект поможет сохранить его, даже если последующие слои вносят дополнительные преобразования.

Пример реализации
Представим, что у нас есть блок нейронной сети, который вычисляет некоторую функцию 
𝐹
(
𝑥
)
F(x) (например, последовательность операций: линейное преобразование, нормализация и нелинейность). Без residual-коннектов блок выдавал бы результат 
𝐹
(
𝑥
)
F(x). При использовании residual-соединения окончательный результат блока будет:

𝑦
=
𝐹
(
𝑥
)
+
𝑥
y=F(x)+x
Если размерности 
𝑥
x и 
𝐹
(
𝑥
)
F(x) совпадают, то операция сложения выполняется напрямую. Если они различаются, часто применяется дополнительное линейное преобразование к 
𝑥
x (например, с помощью матрицы 
𝑊
𝑠
W 
s
​
 ) так, чтобы сделать его размерность совместимой:

𝑦
=
𝐹
(
𝑥
)
+
𝑊
𝑠
𝑥
y=F(x)+W 
s
​
 x
Residual-коннекты в Transformer
В архитектуре Transformer residual-коннекты применяются в каждом слое как в энкодере, так и в декодере. После каждого основного блока (например, после блока multi-head attention или после feed-forward слоя) к выходу этого блока прибавляется входной сигнал, после чего результат нормализуется с помощью layer normalization. Такая схема помогает стабилизировать обучение и обеспечивает лучшее распространение информации через сеть, что критически важно при работе с очень глубокими моделями.

Итог
Residual-коннекты – это мощный инструмент, позволяющий:

Избежать проблем с исчезающим градиентом в глубоких сетях.
Облегчить обучение, позволяя модели фокусироваться на изучении отклонения от тождественного преобразования.
Улучшить передачу и сохранение информации между слоями.
Эта концепция оказалась настолько эффективной, что стала стандартным приёмом при проектировании современных нейросетевых архитектур.